{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddf6a31c",
   "metadata": {},
   "source": [
    "# IDF Training Notebook\n",
    "\n",
    "Train an Integer Discrete Flow (IDF) model for lossless image compression on ImageNet-1k.\n",
    "\n",
    "This notebook uses random 64×64 crops from the streaming ImageNet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae190a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: Tesla T4\n",
      "Memory: 15.8 GB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# COLAB SETUP - Run this cell first!\n",
    "# ============================================================\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if running on Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install dependencies first\n",
    "    print('Installing dependencies...')\n",
    "    %pip install -q datasets huggingface_hub tqdm\n",
    "    \n",
    "    # OPTION 1: Clone from GitHub (if you have the repo there)\n",
    "    # Uncomment and replace with your repo URL:\n",
    "    # !git clone https://github.com/YOUR_USERNAME/AI-Compression.git\n",
    "    # os.chdir('AI-Compression')\n",
    "    \n",
    "    # OPTION 2: Mount Google Drive (if you uploaded the project there)\n",
    "    # from google.colab import drive\n",
    "    # drive.mount('/content/drive')\n",
    "    # os.chdir('/content/drive/MyDrive/AI-Compression')\n",
    "    \n",
    "    # OPTION 3: Upload files manually (simplest for testing)\n",
    "    # Upload the 'src' folder to Colab using the file browser on the left\n",
    "    \n",
    "    print(f'Working directory: {os.getcwd()}')\n",
    "    print(f'Contents: {os.listdir(\".\")}')\n",
    "    \n",
    "    # Check if src folder exists\n",
    "    if not os.path.exists('src'):\n",
    "        print('\\n⚠️  WARNING: src folder not found!')\n",
    "        print('Please either:')\n",
    "        print('  1. Upload the \"src\" folder using the Colab file browser (left panel)')\n",
    "        print('  2. Or clone your GitHub repo (uncomment OPTION 1 above)')\n",
    "        print('  3. Or mount Google Drive (uncomment OPTION 2 above)')\n",
    "\n",
    "# Add current directory to path\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da663ef8",
   "metadata": {},
   "source": [
    "## 1. Setup Dataset\n",
    "\n",
    "Create dataloaders that stream ImageNet and extract random 64×64 crops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21a31ca7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-206984557.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomCropDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mCROP_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from src.dataset.crop_dataset import create_dataloader, RandomCropDataset\n",
    "\n",
    "# Configuration\n",
    "CROP_SIZE = 64\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4  # Reduce if you encounter issues\n",
    "\n",
    "# Create dataloaders\n",
    "print('Creating training dataloader...')\n",
    "train_loader = create_dataloader(\n",
    "    split='train',\n",
    "    crop_size=CROP_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    buffer_size=1000,\n",
    ")\n",
    "\n",
    "print('Creating validation dataloader...')\n",
    "val_loader = create_dataloader(\n",
    "    split='validation',\n",
    "    crop_size=CROP_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    buffer_size=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b78cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some training samples\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f'Batch shape: {sample_batch.shape}')\n",
    "print(f'Value range: [{sample_batch.min():.1f}, {sample_batch.max():.1f}]')\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < len(sample_batch):\n",
    "        img = sample_batch[i].permute(1, 2, 0).numpy() / 255.0\n",
    "        ax.imshow(img.clip(0, 1))\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'Sample {i+1}')\n",
    "plt.suptitle('Random 64×64 Crops from ImageNet', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf75aae3",
   "metadata": {},
   "source": [
    "## 2. Create Model\n",
    "\n",
    "Initialize the Integer Discrete Flow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045e40b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import create_idf_model\n",
    "\n",
    "# Model configuration\n",
    "model_config = {\n",
    "    'in_channels': 3,\n",
    "    'hidden_channels': 64,    # Width of coupling networks\n",
    "    'num_levels': 3,           # Hierarchical levels (squeeze + flow block)\n",
    "    'num_steps': 8,            # Flow steps per level\n",
    "}\n",
    "\n",
    "model = create_idf_model(model_config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Model parameters: {num_params:,}')\n",
    "print(f'Model config: {model_config}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea77c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "test_batch = sample_batch[:4].to(device)\n",
    "print(f'Input shape: {test_batch.shape}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    loss, bpd = model.compute_loss(test_batch)\n",
    "    print(f'Initial BPD: {bpd.item():.3f}')\n",
    "    print(f'(Lower is better, theoretical minimum ~4-5 bpd for natural images)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6135f86e",
   "metadata": {},
   "source": [
    "## 3. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ba4e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train import IDFTrainer\n",
    "\n",
    "# Training configuration\n",
    "LEARNING_RATE = 1e-4\n",
    "GRAD_CLIP = 1.0\n",
    "CHECKPOINT_DIR = '../checkpoints'\n",
    "\n",
    "trainer = IDFTrainer(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    lr=LEARNING_RATE,\n",
    "    grad_clip=GRAD_CLIP,\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    "    use_amp=True,  # Use mixed precision if available\n",
    ")\n",
    "\n",
    "print(f'Trainer initialized')\n",
    "print(f'Checkpoints will be saved to: {CHECKPOINT_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954dc86c",
   "metadata": {},
   "source": [
    "## 4. Train the Model\n",
    "\n",
    "Run the training loop. You can interrupt at any time - checkpoints are saved periodically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da59c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "TOTAL_STEPS = 50000     # Adjust based on compute budget\n",
    "LOG_INTERVAL = 100      # Log every N steps\n",
    "VAL_INTERVAL = 1000     # Validate every N steps\n",
    "SAVE_INTERVAL = 5000    # Save checkpoint every N steps\n",
    "\n",
    "# Start training\n",
    "history = trainer.train(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_steps=TOTAL_STEPS,\n",
    "    log_interval=LOG_INTERVAL,\n",
    "    val_interval=VAL_INTERVAL,\n",
    "    save_interval=SAVE_INTERVAL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a171d0e",
   "metadata": {},
   "source": [
    "## 5. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0177b82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "if len(history) > 0:\n",
    "    steps = [h['step'] for h in history]\n",
    "    bpds = [h['bpd'] for h in history]\n",
    "    lrs = [h['lr'] for h in history]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # BPD plot\n",
    "    axes[0].plot(steps, bpds)\n",
    "    axes[0].set_xlabel('Step')\n",
    "    axes[0].set_ylabel('Bits per Dimension (BPD)')\n",
    "    axes[0].set_title('Training Loss')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning rate plot\n",
    "    axes[1].plot(steps, lrs)\n",
    "    axes[1].set_xlabel('Step')\n",
    "    axes[1].set_ylabel('Learning Rate')\n",
    "    axes[1].set_title('Learning Rate Schedule')\n",
    "    axes[1].set_yscale('log')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'Final BPD: {bpds[-1]:.4f}')\n",
    "    print(f'Best BPD: {min(bpds):.4f}')\n",
    "else:\n",
    "    print('No training history yet.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024c03de",
   "metadata": {},
   "source": [
    "## 6. Test Compression / Reconstruction\n",
    "\n",
    "Verify the model can perfectly reconstruct images (lossless)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9debb7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test lossless reconstruction\n",
    "model.eval()\n",
    "\n",
    "test_batch = next(iter(val_loader))[:4].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Compress\n",
    "    latents, prior_params = model.compress(test_batch)\n",
    "    \n",
    "    # Decompress\n",
    "    reconstructed = model.decompress(latents)\n",
    "    \n",
    "# Check reconstruction error\n",
    "error = (test_batch - reconstructed).abs()\n",
    "max_error = error.max().item()\n",
    "mean_error = error.mean().item()\n",
    "\n",
    "print(f'Max reconstruction error: {max_error:.6f}')\n",
    "print(f'Mean reconstruction error: {mean_error:.6f}')\n",
    "\n",
    "if max_error < 1e-4:\n",
    "    print('✓ Reconstruction is lossless!')\n",
    "else:\n",
    "    print('⚠ Reconstruction has errors (expected for untrained model)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f320acea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize original vs reconstructed\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "\n",
    "for i in range(4):\n",
    "    # Original\n",
    "    orig = test_batch[i].cpu().permute(1, 2, 0).numpy() / 255.0\n",
    "    axes[0, i].imshow(orig.clip(0, 1))\n",
    "    axes[0, i].set_title(f'Original {i+1}')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Reconstructed\n",
    "    recon = reconstructed[i].cpu().permute(1, 2, 0).numpy() / 255.0\n",
    "    axes[1, i].imshow(recon.clip(0, 1))\n",
    "    axes[1, i].set_title(f'Reconstructed {i+1}')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('Lossless Compression Test', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063f39bc",
   "metadata": {},
   "source": [
    "## 7. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdfe5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "trainer.save_checkpoint('notebook_final.pt')\n",
    "print('Model saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f472968",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resume Training (Optional)\n",
    "\n",
    "To resume from a checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cd75fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to resume from checkpoint\n",
    "# trainer.load_checkpoint('../checkpoints/checkpoint_step10000.pt')\n",
    "# \n",
    "# # Continue training\n",
    "# history = trainer.train(\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     num_steps=100000,\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
